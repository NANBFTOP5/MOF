{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import pickle\n",
    "import keras\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D, CuDNNLSTM, Flatten\n",
    "from keras.layers import AveragePooling1D, MaxPooling1D, Bidirectional, GlobalMaxPool1D, Concatenate, GlobalAveragePooling1D, GlobalMaxPooling1D,concatenate\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding1D, BatchNormalization, Flatten, Conv1D, AveragePooling1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.models import Model, load_model\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from keras import backend as K\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(42)\n",
    "random.seed(12345)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "tf.set_random_seed(1234)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('new_data.pickle', 'rb') as f:\n",
    "    Data = pickle.load(f)\n",
    "    Data = pd.DataFrame(Data)\n",
    "\n",
    "# Train test split\n",
    "idx = random.sample(range(2418), 480) # 80% for training\n",
    "all_index = list(np.setdiff1d(range(2418), idx))\n",
    "\n",
    "# Getting X_train, X_val, y_train and scale them\n",
    "\n",
    "X_train = Data.iloc[all_index,:].drop(2251, axis = 1)\n",
    "X_val = Data.iloc[idx,:].drop(2251, axis = 1)\n",
    "y_train = pd.get_dummies(Data.iloc[all_index,:][2251])\n",
    "\n",
    "# X_train = Data.iloc[all_index,:].drop('label', axis = 1)\n",
    "# X_val = Data.iloc[idx,:].drop('label', axis = 1)\n",
    "# y_train = pd.get_dummies(Data.iloc[all_index,:]['label'])\n",
    "# print(y_train)\n",
    "names = y_train.columns\n",
    "mapping = {}\n",
    "i = 0\n",
    "for n in names:\n",
    "    mapping[i] = n\n",
    "    i+=1\n",
    "\n",
    "# # scaler = StandardScaler()\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(X, f, filters, stage, block):\n",
    "    \"\"\"\n",
    "    实现图3的恒等块\n",
    "\n",
    "    参数：\n",
    "        X - 输入的tensor类型的数据，维度为( m, 1001， 1)\n",
    "        f - 整数，指定主路径中间的CONV窗口的维度\n",
    "        filters - 整数列表，定义了主路径每层的卷积层的过滤器数量\n",
    "        stage - 整数，根据每层的位置来命名每一层，与block参数一起使用。\n",
    "        block - 字符串，据每层的位置来命名每一层，与stage参数一起使用。\n",
    "\n",
    "    返回：\n",
    "        X - 恒等块的输出，tensor类型，维度为(1001，1)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #定义命名规则\n",
    "    conv_name_base = \"res\" + str(stage) + block + \"_branch\"\n",
    "    bn_name_base   = \"bn\"  + str(stage) + block + \"_branch\"\n",
    "\n",
    "    #获取过滤器\n",
    "    F1, F2, F3 = filters\n",
    "\n",
    "    #保存输入数据，将会用于为主路径添加捷径\n",
    "    X_shortcut = X\n",
    "\n",
    "    #主路径的第一部分\n",
    "    ##卷积层\n",
    "    X = Conv1D(filters=F1, kernel_size=1, strides=1 ,padding=\"valid\",\n",
    "               name=conv_name_base+\"2a\", kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    ##归一化\n",
    "    X = BatchNormalization(name=bn_name_base+\"2a\")(X)\n",
    "    ##使用ReLU激活函数\n",
    "    X = Activation(\"relu\")(X)\n",
    "\n",
    "    #主路径的第二部分\n",
    "    ##卷积层\n",
    "    X = Conv1D(filters=F2, kernel_size=f,strides=1, padding=\"same\",\n",
    "               name=conv_name_base+\"2b\", kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    ##归一化\n",
    "    X = BatchNormalization(name=bn_name_base+\"2b\")(X)\n",
    "    ##使用ReLU激活函数\n",
    "    X = Activation(\"relu\")(X)\n",
    "\n",
    "\n",
    "    #主路径的第三部分\n",
    "    ##卷积层\n",
    "    X = Conv1D(filters=F3, kernel_size=1, strides=1, padding=\"valid\",\n",
    "               name=conv_name_base+\"2c\", kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    ##归一化\n",
    "    X = BatchNormalization(name=bn_name_base+\"2c\")(X)\n",
    "    ##没有ReLU激活函数\n",
    "\n",
    "    #最后一步：\n",
    "    ##将捷径与输入加在一起\n",
    "    X = Add()([X,X_shortcut])\n",
    "    ##使用ReLU激活函数\n",
    "    X = Activation(\"relu\")(X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_block(X, f, filters, stage, block, s=2):\n",
    "    #定义命名规则\n",
    "    conv_name_base = \"res\" + str(stage) + block + \"_branch\"\n",
    "    bn_name_base   = \"bn\"  + str(stage) + block + \"_branch\"\n",
    "\n",
    "    #获取过滤器数量\n",
    "    F1, F2, F3 = filters\n",
    "\n",
    "    #保存输入数据\n",
    "    X_shortcut = X\n",
    "    \n",
    "    #主路径\n",
    "    ##主路径第一部分\n",
    "    X = Conv1D(filters=F1, kernel_size=1, strides=s, padding=\"valid\",\n",
    "               name=conv_name_base+\"2a\", kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(name=bn_name_base+\"2a\")(X)\n",
    "    X = Activation(\"relu\")(X)\n",
    "\n",
    "    ##主路径第二部分\n",
    "    X = Conv1D(filters=F2, kernel_size=f, strides=1, padding=\"same\",\n",
    "               name=conv_name_base+\"2b\", kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(name=bn_name_base+\"2b\")(X)\n",
    "    X = Activation(\"relu\")(X)\n",
    "\n",
    "    ##主路径第三部分\n",
    "    X = Conv1D(filters=F3, kernel_size=1, strides=1, padding=\"valid\",\n",
    "               name=conv_name_base+\"2c\", kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(name=bn_name_base+\"2c\")(X)\n",
    "\n",
    "    #捷径\n",
    "    X_shortcut = Conv1D(filters=F3, kernel_size=1, strides=s, padding=\"valid\",\n",
    "               name=conv_name_base+\"1\", kernel_initializer=glorot_uniform(seed=0))(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(name=bn_name_base+\"1\")(X_shortcut)\n",
    "\n",
    "    #最后一步\n",
    "    X = Add()([X,X_shortcut])\n",
    "    X = Activation(\"relu\")(X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct ResNet50\n",
    "def ResNet50(input_shape=(2251,1),classes=186):\n",
    "    #定义tensor类型的输入数据\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    #0填充\n",
    "    X = keras.layers.convolutional.ZeroPadding1D(3)(X_input)\n",
    "\n",
    "    #stage1\n",
    "    X = Conv1D(filters=64, kernel_size=7, strides=2, name=\"conv1\",\n",
    "               kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(name=\"bn_conv1\")(X)\n",
    "    X = Activation(\"relu\")(X)\n",
    "    X = MaxPooling1D(pool_size=3, strides=2)(X)\n",
    "\n",
    "    #stage2\n",
    "    X = convolutional_block(X, f=3, filters=[64,64,256], stage=2, block=\"a\", s=1)\n",
    "    X = identity_block(X, f=3, filters=[64,64,256], stage=2, block=\"b\")\n",
    "    X = identity_block(X, f=3, filters=[64,64,256], stage=2, block=\"c\")\n",
    "\n",
    "    #stage3\n",
    "    X = convolutional_block(X, f=3, filters=[128,128,512], stage=3, block=\"a\", s=2)\n",
    "    X = identity_block(X, f=3, filters=[128,128,512], stage=3, block=\"b\")\n",
    "    X = identity_block(X, f=3, filters=[128,128,512], stage=3, block=\"c\")\n",
    "    X = identity_block(X, f=3, filters=[128,128,512], stage=3, block=\"d\")\n",
    "\n",
    "    #stage4\n",
    "    X = convolutional_block(X, f=3, filters=[256,256,1024], stage=4, block=\"a\", s=2)\n",
    "    X = identity_block(X, f=3, filters=[256,256,1024], stage=4, block=\"b\")\n",
    "    X = identity_block(X, f=3, filters=[256,256,1024], stage=4, block=\"c\")\n",
    "    X = identity_block(X, f=3, filters=[256,256,1024], stage=4, block=\"d\")\n",
    "    X = identity_block(X, f=3, filters=[256,256,1024], stage=4, block=\"e\")\n",
    "    X = identity_block(X, f=3, filters=[256,256,1024], stage=4, block=\"f\")\n",
    "\n",
    "    #stage5\n",
    "    X = convolutional_block(X, f=3, filters=[512,512,2048], stage=5, block=\"a\", s=2)\n",
    "    X = identity_block(X, f=3, filters=[512,512,2048], stage=5, block=\"b\")\n",
    "    X = identity_block(X, f=3, filters=[512,512,2048], stage=5, block=\"c\")\n",
    "\n",
    "    #均值池化层\n",
    "    X = AveragePooling1D(pool_size=2,padding=\"same\")(X)\n",
    "\n",
    "    #输出层\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, activation=\"softmax\", name=\"fc\"+str(classes),\n",
    "              kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "\n",
    "\n",
    "    #创建模型\n",
    "    cnn_model = Model(inputs=X_input, outputs=X, name=\"ResNet50\")\n",
    "\n",
    "    return cnn_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = ResNet50(input_shape=(2251,1),classes=186)\n",
    "cnn_model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yanghaoyue/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "1938/1938 [==============================] - 28s 14ms/step - loss: 15.7774 - acc: 0.0108\n",
      "Epoch 2/100\n",
      "1938/1938 [==============================] - 14s 7ms/step - loss: 15.8852 - acc: 0.0144\n",
      "Epoch 3/100\n",
      "1938/1938 [==============================] - 14s 7ms/step - loss: 15.8852 - acc: 0.0144\n",
      "Epoch 4/100\n",
      "1728/1938 [=========================>....] - ETA: 1s - loss: 15.9036 - acc: 0.0133"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-2fa899afb170>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fitting CNN model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1938\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2251\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Validating the CNN modela\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m480\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2251\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fitting CNN model\n",
    "cnn_model.fit(X_train.reshape(1938, 2251, 1), y_train.values, batch_size = 32, epochs = 100, verbose = 1)\n",
    "\n",
    "# Validating the CNN modela\n",
    "pred = cnn_model.predict(X_val.reshape(480, 2251, 1))\n",
    "out = [np.argmax(p) for p in pred]\n",
    "out = np.array(out)\n",
    "\n",
    "# Create category mapping dictionary\n",
    "y_val = Data.iloc[idx,:][2251]\n",
    "    \n",
    "# Calculating accuracy\n",
    "predicted = np.vectorize(mapping.get)(out)\n",
    "print('Accuracy: '+ str(round(np.sum(predicted == y_val)/480,2) * 100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ZIF-90', 'MOF-5', 'ZIF-65', 'BAHGUN03', 'ZIF-8', 'AFIXES', 'HUYKUH', 'VOCQAF']\n",
      "['ZIF-90', 'MOF-5', 'BAHGUN03', 'ZIF-65', 'BAHGUN01', 'UHAXUW', 'NICZAA01', 'VOCQAF']\n",
      "['HUJVOX', 'LAMHAI', 'F-MOF-8', 'ZUVTEP', 'MIHBAG', 'LEJBUX', 'PUTYAE', 'JOSNAG']\n",
      "['ZIF-90', 'MOF-5', 'ZIF-65', 'DEFKUU', 'BAHGUN03', 'ZIF-8', 'UHAXUW', 'BAHGUN01']\n",
      "['PUTYEI', 'UGUQOC', 'ZIF-12', 'NUMMOX', 'MOF-5', 'WABVIF02', 'ZIF-71', 'UHAXUW']\n",
      "['ZIF-12', 'ZIF-71', 'DEFKUU', 'PUTYEI', 'MIL-101', 'IDIWOH03', 'UGUQOC', 'ZIF-90']\n",
      "['IDIWOH03', 'DEFKUU', 'ZIF-71', 'MIL-101', 'UGUQOC', 'ZIF-12', 'IDIWOH01', 'PUTYEI']\n",
      "['ZIF-90', 'MOF-5', 'IDIWOH03', 'ZIF-65', 'ZIF-8', 'DEFKUU', 'JOSNAG01', 'BAHGUN03']\n",
      "['MOF-5', 'ZIF-90', 'ZIF-8', 'IDIWOH03', 'ZIF-65', 'AFIXES', 'HUYKUH', 'UHAXUW']\n",
      "['MOF-5', 'ZIF-90', 'ZIF-8', 'UHAXUW', 'AFIXES', 'HUYKUH', 'BAHGUN03', 'PUTYEI']\n",
      "['MOF-5', 'ZIF-90', 'ZIF-8', 'IDIWOH03', 'UHAXUW', 'ZIF-65', 'AFIXES', 'HUYKUH']\n",
      "['ZIF-90', 'MOF-5', 'AFIXES', 'ZIF-65', 'ZIF-8', 'BAHGUN03', 'IDIWOH03', 'ITUVEZ']\n",
      "['ZIF-90', 'MOF-5', 'ZIF-65', 'IDIWOH03', 'ZIF-8', 'AFIXES', 'UHAXUW', 'HUYKUH']\n",
      "['ZIF-90', 'MOF-5', 'AFIXES', 'ZIF-65', 'ZIF-8', 'ITUVEZ', 'BAHGUN03', 'IDIWOH03']\n"
     ]
    }
   ],
   "source": [
    "# Load and test on the experimental data\n",
    "with open('norm_T_origin.pickle', 'rb') as f:\n",
    "    Test = pickle.load(f)\n",
    "    Test = pd.DataFrame(Test)\n",
    "X_test = scaler.transform(Test.iloc[:,:])\n",
    "pred = cnn_model.predict(X_test.reshape(14, 2251, 1))\n",
    "out = [np.argmax(p) for p in pred]\n",
    "out = np.array(out)\n",
    "predicted = np.vectorize(mapping.get)(out)\n",
    "# print(predicted)\n",
    "\n",
    "preds = []\n",
    "for p in pred:\n",
    "  P = []\n",
    "  for i in np.argsort(p)[::-1][0:8]:\n",
    "    P.append(mapping[i])\n",
    "  preds.append(P)\n",
    "\n",
    "for p in preds:\n",
    "  print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "# cnn_model.save(\"cnn_3-zif8-in-top3-3-zif90-in-top1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['JARMEU10', 'DUT-7', 'UGUQOC', 'JARMEU', 'ZIF-67', 'ZIF-71', 'ZIF-65', 'GORQAF']\n",
      "['IDIWOH', 'ZIF-67', 'JARMEU10', 'DUT-7', 'JARMEU', 'UGUQOC', 'IDIWOH07', 'GORQAF']\n",
      "['JOSNAG', 'NUMMOX', 'WABVIF04', 'JARMEU10', 'ZIF-75', 'REGJIW', 'TILVOZ', 'JARMEU']\n",
      "['JARMEU10', 'UGUQOC', 'IDIWOH', 'DUT-23', 'JARMEU', 'HUYKIV', 'QOJTUE', 'ZIF-6']\n",
      "['AFEHUO', 'ZIF-70', 'QEFNAQ', 'ZIF-71', 'SAJRAW10', 'HUKRIO', 'EDIKEH', 'QAYTOZ']\n",
      "['AFEHUO', 'ZIF-70', 'QEFNAQ', 'QAYTOZ', 'SAJRAW10', 'HUKRIO', 'JARMEU10', 'ZIF-71']\n",
      "['AFEHUO', 'ZIF-70', 'SAJRAW10', 'YEFPAA', 'QEFNAQ', 'HUKRIO', 'QAYTOZ', 'XAYMUF']\n",
      "['ZIF-90', 'MOF-5', 'IDIWOH03', 'ZIF-65', 'ZIF-8', 'DEFKUU', 'JOSNAG01', 'BAHGUN03']\n",
      "['MOF-5', 'ZIF-90', 'ZIF-8', 'IDIWOH03', 'ZIF-65', 'AFIXES', 'HUYKUH', 'UHAXUW']\n",
      "['MOF-5', 'ZIF-90', 'ZIF-8', 'UHAXUW', 'AFIXES', 'HUYKUH', 'BAHGUN03', 'PUTYEI']\n",
      "['MOF-5', 'ZIF-90', 'ZIF-8', 'IDIWOH03', 'UHAXUW', 'ZIF-65', 'AFIXES', 'HUYKUH']\n",
      "['ZIF-90', 'MOF-5', 'AFIXES', 'ZIF-65', 'ZIF-8', 'BAHGUN03', 'IDIWOH03', 'ITUVEZ']\n",
      "['ZIF-90', 'MOF-5', 'ZIF-65', 'IDIWOH03', 'ZIF-8', 'AFIXES', 'UHAXUW', 'HUYKUH']\n",
      "['ZIF-90', 'MOF-5', 'AFIXES', 'ZIF-65', 'ZIF-8', 'ITUVEZ', 'BAHGUN03', 'IDIWOH03']\n"
     ]
    }
   ],
   "source": [
    "# Test_first=np.load('fft_data14.npy')\n",
    "# Test_first = Test_first[:7,:2251]\n",
    "# Test_first = pd.DataFrame(Test_first)\n",
    "# # print(Test_first.shape)\n",
    "# with open('norm_T_origin.pickle', 'rb') as f:\n",
    "#     Test_last = pickle.load(f)\n",
    "#     Test_last = pd.DataFrame(Test_last)\n",
    "# Test_last = Test_last.iloc[7:,:]\n",
    "# # print(Test_last.shape)\n",
    "# Test = Test_first.append(Test_last,ignore_index=True)\n",
    "# Test.shape\n",
    "\n",
    "Test = pd.read_csv('mix_test.csv')\n",
    "\n",
    "X_test = scaler.transform(Test.iloc[:,1:])\n",
    "pred = cnn_model.predict(X_test.reshape(14, 2251, 1))\n",
    "out = [np.argmax(p) for p in pred]\n",
    "out = np.array(out)\n",
    "predicted = np.vectorize(mapping.get)(out)\n",
    "# print(predicted)\n",
    "\n",
    "preds = []\n",
    "for p in pred:\n",
    "  P = []\n",
    "  for i in np.argsort(p)[::-1][0:8]:\n",
    "    P.append(mapping[i])\n",
    "  preds.append(P)\n",
    "\n",
    "for p in preds:\n",
    "  print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test.to_csv('mix_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
