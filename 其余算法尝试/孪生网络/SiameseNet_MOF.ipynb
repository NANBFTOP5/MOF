{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import pickle\n",
    "import keras\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D, CuDNNLSTM, Flatten\n",
    "from keras.layers import Lambda, AveragePooling1D, MaxPooling1D, Bidirectional, GlobalMaxPool1D, Concatenate, GlobalAveragePooling1D, GlobalMaxPooling1D,concatenate\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding1D, BatchNormalization, Flatten, Conv1D, AveragePooling1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import SGD,Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from keras import backend as K\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(42)\n",
    "random.seed(12345)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "tf.set_random_seed(1234)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yanghaoyue/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "123995329"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "def W_init(shape,name=None):\n",
    "    \"\"\"Initialize weights as in paper\"\"\"\n",
    "    values = np.random.normal(loc=0,scale=1e-2,size=shape)\n",
    "    return K.variable(values,name=name)\n",
    "#//TODO: figure out how to initialize layer biases in keras.\n",
    "def b_init(shape,name=None):\n",
    "    \"\"\"Initialize bias as in paper\"\"\"\n",
    "    values=np.random.normal(loc=0.5,scale=1e-2,size=shape)\n",
    "    return K.variable(values,name=name)\n",
    "\n",
    "input_shape = (1001, 1)\n",
    "left_input = Input(input_shape)\n",
    "right_input = Input(input_shape)\n",
    "#build convnet to use in each siamese 'leg'\n",
    "convnet = Sequential()\n",
    "convnet.add(Conv1D(64,10,activation='relu',input_shape=input_shape,\n",
    "                   kernel_initializer=W_init,kernel_regularizer=l2(2e-4)))\n",
    "convnet.add(MaxPooling1D())\n",
    "convnet.add(Conv1D(128,7,activation='relu',\n",
    "                   kernel_regularizer=l2(2e-4),kernel_initializer=W_init,bias_initializer=b_init))\n",
    "convnet.add(MaxPooling1D())\n",
    "convnet.add(Conv1D(128,4,activation='relu',kernel_initializer=W_init,kernel_regularizer=l2(2e-4),bias_initializer=b_init))\n",
    "convnet.add(MaxPooling1D())\n",
    "convnet.add(Conv1D(256,4,activation='relu',kernel_initializer=W_init,kernel_regularizer=l2(2e-4),bias_initializer=b_init))\n",
    "convnet.add(Flatten())\n",
    "convnet.add(Dense(4096,activation=\"sigmoid\",kernel_regularizer=l2(1e-3),kernel_initializer=W_init,bias_initializer=b_init))\n",
    "\n",
    "#call the convnet Sequential model on each of the input tensors so params will be shared\n",
    "encoded_l = convnet(left_input)\n",
    "encoded_r = convnet(right_input)\n",
    "#layer to merge two encoded inputs with the l1 distance between them\n",
    "L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "#call this layer on list of two input tensors.\n",
    "L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "prediction = Dense(1,activation='sigmoid',bias_initializer=b_init)(L1_distance)\n",
    "siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "\n",
    "optimizer = Adam(0.00006)\n",
    "#//TODO: get layerwise learning rates and momentum annealing scheme described in paperworking\n",
    "siamese_net.compile(loss=\"binary_crossentropy\",optimizer=optimizer)\n",
    "\n",
    "siamese_net.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_read = open('new_data.pickle', 'rb')\n",
    "\n",
    "#通过pickle的load函数读取data1.pkl中的对象，并赋值给data2\n",
    "tmp = pickle.load(file_to_read)\n",
    "\n",
    "y_train = pd.get_dummies(Data.iloc[all_index,:]['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = Sequential()\n",
    "cnn_model.add(Conv1D(6, 5, strides = 1, activation = 'relu', input_shape = (1001, 1)))\n",
    "cnn_model.add(MaxPooling1D(2, strides = 2))\n",
    "cnn_model.add(Dropout(0.2))\n",
    "cnn_model.add(Conv1D(16, 5, strides = 1, activation = 'relu'))\n",
    "cnn_model.add(MaxPooling1D(2, strides = 2))\n",
    "cnn_model.add(Dropout(0.2))\n",
    "cnn_model.add(Conv1D(32, 5, strides = 1, activation = 'relu'))\n",
    "cnn_model.add(MaxPooling1D(2, strides = 2))\n",
    "cnn_model.add(Dropout(0.2))\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(120, activation = 'relu'))\n",
    "cnn_model.add(Dense(84, activation = 'relu'))\n",
    "cnn_model.add(Dense(186, activation = 'softmax', activity_regularizer = keras.regularizers.l2(0.1)))\n",
    "cnn_model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.fit(X_train.reshape(118880, 1001, 1), y_train.values, batch_size = 64, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 搭建SiameseNet模型\n",
    "基本上是加深版本的Lenet 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1088141"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = (2251, 1)\n",
    "left_input = Input(input_shape)\n",
    "right_input = Input(input_shape)\n",
    "#build convnet to use in each siamese 'leg'\n",
    "\n",
    "# Construct CNN layers\n",
    "\n",
    "# cnn_model = Sequential()\n",
    "# cnn_model.add(Conv1D(64, 3, strides = 1, activation = 'relu', input_shape = (2251, 1)))\n",
    "# cnn_model.add(Conv1D(64, 3, strides = 1, activation = 'relu'))\n",
    "# cnn_model.add(MaxPooling1D(2, strides = 2))\n",
    "# cnn_model.add(Dropout(0.2))\n",
    "\n",
    "# cnn_model.add(Conv1D(128, 3, strides = 1, activation = 'relu'))\n",
    "# cnn_model.add(Conv1D(128, 3, strides = 1, activation = 'relu'))\n",
    "# cnn_model.add(MaxPooling1D(2, strides = 2))\n",
    "# cnn_model.add(Dropout(0.2))\n",
    "\n",
    "# cnn_model.add(Conv1D(256, 3, strides = 1, activation = 'relu'))\n",
    "# # cnn_model.add(Conv1D(256, 3, strides = 1, activation = 'relu'))\n",
    "# cnn_model.add(MaxPooling1D(2, strides = 2))\n",
    "# cnn_model.add(Dropout(0.2))\n",
    "\n",
    "# cnn_model.add(Conv1D(512, 3, strides = 1, activation = 'relu'))\n",
    "# #cnn_model.add(Conv1D(512, 3, strides = 1, activation = 'relu'))\n",
    "# cnn_model.add(MaxPooling1D(2, strides = 2))\n",
    "# cnn_model.add(Dropout(0.2))\n",
    "\n",
    "# cnn_model.add(Flatten())\n",
    "# cnn_model.add(Dense(120, activation = 'relu'))\n",
    "# cnn_model.add(Dense(84, activation = 'relu'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Lenet 5\n",
    "\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Conv1D(6, 5, strides = 1, activation = 'relu', input_shape = (2251, 1)))\n",
    "cnn_model.add(MaxPooling1D(2, strides = 2))\n",
    "cnn_model.add(Dropout(0.2))\n",
    "\n",
    "cnn_model.add(Conv1D(16, 5, strides = 1, activation = 'relu'))\n",
    "cnn_model.add(Conv1D(16, 5, strides = 1, activation = 'relu'))\n",
    "cnn_model.add(MaxPooling1D(2, strides = 2))\n",
    "cnn_model.add(Dropout(0.2))\n",
    "\n",
    "cnn_model.add(Conv1D(32, 5, strides = 1, activation = 'relu'))\n",
    "cnn_model.add(Conv1D(32, 5, strides = 1, activation = 'relu'))\n",
    "cnn_model.add(MaxPooling1D(2, strides = 2))\n",
    "cnn_model.add(Dropout(0.2))\n",
    "\n",
    "cnn_model.add(Conv1D(64, 5, strides = 1, activation = 'relu'))\n",
    "cnn_model.add(Conv1D(64, 5, strides = 1, activation = 'relu'))\n",
    "cnn_model.add(MaxPooling1D(2, strides = 2))\n",
    "cnn_model.add(Dropout(0.2))\n",
    "\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(120, activation = 'relu'))\n",
    "cnn_model.add(Dense(84, activation = 'relu'))\n",
    "cnn_model.add(Dense(186, activation = 'softmax', activity_regularizer = keras.regularizers.l2(0.1)))\n",
    "\n",
    "#call the convnet Sequential model on each of the input tensors so params will be shared\n",
    "encoded_l = cnn_model(left_input)\n",
    "encoded_r = cnn_model(right_input)\n",
    "#layer to merge two encoded inputs with the l1 distance between them\n",
    "L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "#call this layer on list of two input tensors.\n",
    "L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "#prediction = Dense(1,activation='sigmoid',bias_initializer=b_init)(L1_distance)\n",
    "prediction = Dense(1,activation='sigmoid')(L1_distance)\n",
    "siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "\n",
    "optimizer = Adam(0.00016)\n",
    "#//TODO: get layerwise learning rates and momentum annealing scheme described in paperworking\n",
    "siamese_net.compile(loss=\"binary_crossentropy\",optimizer=optimizer)\n",
    "\n",
    "siamese_net.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_8 (Conv1D)            (None, 2247, 6)           36        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 1123, 6)           0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1123, 6)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 1119, 16)          496       \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 1115, 16)          1296      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 557, 16)           0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 557, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 553, 32)           2592      \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 549, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 274, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 274, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 270, 64)           10304     \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 266, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 133, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 133, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8512)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 120)               1021560   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 186)               15810     \n",
      "=================================================================\n",
      "Total params: 1,087,954\n",
      "Trainable params: 1,087,954\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 2251, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 2251, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 186)          1087954     input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 186)          0           sequential_2[1][0]               \n",
      "                                                                 sequential_2[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            187         lambda_2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,088,141\n",
      "Trainable params: 1,088,141\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "siamese_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_to_read = open('train_pairs.pickle', 'rb')\n",
    "\n",
    "# train_pairs = pickle.load(file_to_read)\n",
    "\n",
    "\n",
    "\n",
    "# file_to_read = open('train_targets.pickle', 'rb')\n",
    "\n",
    "# train_targets = pickle.load(file_to_read)\n",
    "\n",
    "\n",
    "file_to_read = open('test_pairs.pickle', 'rb')\n",
    "\n",
    "test_pairs = pickle.load(file_to_read)\n",
    "\n",
    "\n",
    "file_to_read = open('test_targets.pickle', 'rb')\n",
    "\n",
    "test_targets = pickle.load(file_to_read)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2418, 2252)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new train data\n",
    "#打开一个名为data1.pkl的文件，打开方式为二进制读取(参数‘rb’)\n",
    "file_to_read = open('new_data.pickle', 'rb')\n",
    "\n",
    "#通过pickle的load函数读取data1.pkl中的对象，并赋值给data2\n",
    "tmp = pickle.load(file_to_read)\n",
    "\n",
    "# train_data = tmp  \n",
    "t = np.array(tmp)\n",
    "t.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87048,)\n",
      "(29016, 3)\n",
      "(2, 29016, 2251)\n",
      "(2, 29016, 2251, 1)\n",
      "(29016,)\n"
     ]
    }
   ],
   "source": [
    "train_targets, train_pairs = [],[]\n",
    "pair_1, pair_2 = [], []\n",
    "merge_label_pair = []\n",
    "\n",
    "# label 1 pairs\n",
    "for n in range(186):\n",
    "    locfix = n*13\n",
    "    for i in range(13):\n",
    "        loc = n*13+i\n",
    "        for j in range(i+1, 13):\n",
    "            merge_label_pair.append(train_data[loc][:-1])\n",
    "            merge_label_pair.append(train_data[locfix+j][:-1])\n",
    "            merge_label_pair.append(1)\n",
    "\n",
    "# merge_label_pair = np.array(merge_label_pair)\n",
    "# print(merge_label_pair.shape)            \n",
    "# print(merge_label_pair[2])            \n",
    "# merge_label_pair = np.array(merge_label_pair).reshape(186*78,3)\n",
    "# print(merge_label_pair.shape)\n",
    "\n",
    "\n",
    "\n",
    "# label 0 pairs\n",
    "\n",
    "for n in range(186):\n",
    "    locfix = n*13\n",
    "    index = np.arange(2418)\n",
    "    delete_index = list(range(n*13,(n+1)*13))\n",
    "    rand_index = np.random.choice(np.delete(index,delete_index,0),6)\n",
    "    for i in range(13):\n",
    "        loc = n*13+i\n",
    "        for j in range(6):\n",
    "            merge_label_pair.append(train_data[loc][:-1])\n",
    "            merge_label_pair.append(train_data[rand_index[j]][:-1])\n",
    "            merge_label_pair.append(0)\n",
    "\n",
    "merge_label_pair = np.array(merge_label_pair)\n",
    "print(merge_label_pair.shape)\n",
    "\n",
    "        \n",
    "merge_label_pair = merge_label_pair.reshape(186*78*2,3)\n",
    "print(merge_label_pair.shape)\n",
    "\n",
    "# print(merge_label_pair[1][1])\n",
    "# a = np.array(merge_label_pair[:][1])\n",
    "# print(a.shape)\n",
    "\n",
    "np.random.shuffle(merge_label_pair)\n",
    "\n",
    "#split\n",
    "pair_1, pair_2 = [], []\n",
    "for i in range(29016):\n",
    "    pair_1.append(merge_label_pair[i][0])\n",
    "    pair_2.append(merge_label_pair[i][1])\n",
    "\n",
    "train_pairs = [pair_1, pair_2]\n",
    "\n",
    "# print(train_pairs)\n",
    "train_pairs = np.array(train_pairs)\n",
    "print(train_pairs.shape)\n",
    "\n",
    "train_pairs = train_pairs.reshape(2,186*78*2,2251,1)\n",
    "print(train_pairs.shape)\n",
    "train_pairs = train_pairs.tolist()\n",
    "\n",
    "\n",
    "train_targets = merge_label_pair[:, -1]\n",
    "\n",
    "\n",
    "# print(train_targets[1])\n",
    "print(train_targets.shape)\n",
    "\n",
    "\n",
    "train_targets = train_targets.tolist()\n",
    "\n",
    "\n",
    "# print(train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_read = open('train_low_None_.pickle', 'rb')\n",
    "\n",
    "tmp1 = pickle.load(file_to_read)\n",
    "\n",
    "train_data = tmp1.values  \n",
    "\n",
    "\n",
    "train_targets, train_pairs = [],[]\n",
    "pair_1, pair_2 = [], []\n",
    "\n",
    "index = np.arange(800)\n",
    "# print(index)\n",
    "\n",
    "rand_index = np.random.choice(index,185)\n",
    "# print(rand_index)\n",
    "for i in range(185):\n",
    "    for j in range(185):\n",
    "        pair_1.append(train_data[rand_index[i]][:-1])\n",
    "        pair_2.append(train_data[rand_index[j]][:-1])\n",
    "        train_targets.append(1)\n",
    "        \n",
    "        pair_1.append(train_data[rand_index[i]][:-1])\n",
    "        pair_2.append(train_data[rand_index[j]+800*(j+1)][:-1])\n",
    "        train_targets.append(0)\n",
    "            \n",
    "            \n",
    "#print(pairs,targets)\n",
    "train_pairs = [pair_1, pair_2]\n",
    "train_pairs = np.array(train_pairs)\n",
    "print(train_pairs.shape)\n",
    "train_pairs = train_pairs.reshape(2,185*185*2,1001,1)\n",
    "print(train_pairs)\n",
    "print(train_pairs.shape)\n",
    "\n",
    "\n",
    "# print(train_pairs[:][:10][:10][:])\n",
    "# train_pairs = train_pairs.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = open('train_pairs.pickle', 'wb')\n",
    "\n",
    "pickle.dump(train_pairs,output)\n",
    "\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = open('train_targets.pickle', 'wb')\n",
    "\n",
    "pickle.dump(train_targets,output)\n",
    "\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RASGAT\n",
      "RASGAT\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(train_pairs[0][0][-1][:])\n",
    "\n",
    "print(train_pairs[1][0][-1][:])\n",
    "\n",
    "print(train_targets[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yanghaoyue/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "29016/29016 [==============================] - 11s 366us/step - loss: 0.7589\n",
      "Epoch 2/100\n",
      "29016/29016 [==============================] - 8s 269us/step - loss: 0.7503\n",
      "Epoch 3/100\n",
      "29016/29016 [==============================] - 8s 273us/step - loss: 0.7357\n",
      "Epoch 4/100\n",
      "29016/29016 [==============================] - 8s 281us/step - loss: 0.7168\n",
      "Epoch 5/100\n",
      "29016/29016 [==============================] - 8s 277us/step - loss: 0.6976\n",
      "Epoch 6/100\n",
      "29016/29016 [==============================] - 8s 275us/step - loss: 0.6792\n",
      "Epoch 7/100\n",
      "29016/29016 [==============================] - 8s 269us/step - loss: 0.6620\n",
      "Epoch 8/100\n",
      "29016/29016 [==============================] - 8s 270us/step - loss: 0.6457\n",
      "Epoch 9/100\n",
      "29016/29016 [==============================] - 8s 272us/step - loss: 0.6295\n",
      "Epoch 10/100\n",
      "29016/29016 [==============================] - 8s 274us/step - loss: 0.6134\n",
      "Epoch 11/100\n",
      "29016/29016 [==============================] - 8s 275us/step - loss: 0.5978\n",
      "Epoch 12/100\n",
      "29016/29016 [==============================] - 8s 276us/step - loss: 0.5826\n",
      "Epoch 13/100\n",
      "29016/29016 [==============================] - 8s 277us/step - loss: 0.5679\n",
      "Epoch 14/100\n",
      "29016/29016 [==============================] - 8s 280us/step - loss: 0.5535\n",
      "Epoch 15/100\n",
      "29016/29016 [==============================] - 8s 279us/step - loss: 0.5393\n",
      "Epoch 16/100\n",
      "29016/29016 [==============================] - 8s 278us/step - loss: 0.5256\n",
      "Epoch 17/100\n",
      "29016/29016 [==============================] - 8s 278us/step - loss: 0.5122\n",
      "Epoch 18/100\n",
      "29016/29016 [==============================] - 8s 278us/step - loss: 0.4994\n",
      "Epoch 19/100\n",
      "29016/29016 [==============================] - 8s 280us/step - loss: 0.4871\n",
      "Epoch 20/100\n",
      "29016/29016 [==============================] - 8s 282us/step - loss: 0.4750\n",
      "Epoch 21/100\n",
      "29016/29016 [==============================] - 8s 282us/step - loss: 0.4634\n",
      "Epoch 22/100\n",
      "29016/29016 [==============================] - 8s 281us/step - loss: 0.4525\n",
      "Epoch 23/100\n",
      "29016/29016 [==============================] - 8s 281us/step - loss: 0.4417\n",
      "Epoch 24/100\n",
      "29016/29016 [==============================] - 8s 281us/step - loss: 0.4316\n",
      "Epoch 25/100\n",
      "29016/29016 [==============================] - 8s 283us/step - loss: 0.4216\n",
      "Epoch 26/100\n",
      "29016/29016 [==============================] - 8s 282us/step - loss: 0.4122\n",
      "Epoch 27/100\n",
      "29016/29016 [==============================] - 8s 280us/step - loss: 0.4030\n",
      "Epoch 28/100\n",
      "29016/29016 [==============================] - 8s 281us/step - loss: 0.3943\n",
      "Epoch 29/100\n",
      "29016/29016 [==============================] - 8s 280us/step - loss: 0.3859\n",
      "Epoch 30/100\n",
      "29016/29016 [==============================] - 8s 280us/step - loss: 0.3776\n",
      "Epoch 31/100\n",
      "29016/29016 [==============================] - 8s 281us/step - loss: 0.3698\n",
      "Epoch 32/100\n",
      "29016/29016 [==============================] - 8s 281us/step - loss: 0.3623\n",
      "Epoch 33/100\n",
      "29016/29016 [==============================] - 8s 280us/step - loss: 0.3549\n",
      "Epoch 34/100\n",
      "29016/29016 [==============================] - 8s 280us/step - loss: 0.3478\n",
      "Epoch 35/100\n",
      "29016/29016 [==============================] - 8s 280us/step - loss: 0.3406\n",
      "Epoch 36/100\n",
      "29016/29016 [==============================] - 8s 281us/step - loss: 0.3336\n",
      "Epoch 37/100\n",
      "29016/29016 [==============================] - 8s 287us/step - loss: 0.3271\n",
      "Epoch 38/100\n",
      "29016/29016 [==============================] - 8s 282us/step - loss: 0.3209\n",
      "Epoch 39/100\n",
      "29016/29016 [==============================] - 8s 283us/step - loss: 0.3146\n",
      "Epoch 40/100\n",
      "29016/29016 [==============================] - 8s 283us/step - loss: 0.3088\n",
      "Epoch 41/100\n",
      "29016/29016 [==============================] - 8s 281us/step - loss: 0.3029\n",
      "Epoch 42/100\n",
      "29016/29016 [==============================] - 8s 283us/step - loss: 0.2969\n",
      "Epoch 43/100\n",
      "29016/29016 [==============================] - 8s 282us/step - loss: 0.2909\n",
      "Epoch 44/100\n",
      "29016/29016 [==============================] - 8s 281us/step - loss: 0.2851\n",
      "Epoch 45/100\n",
      "29016/29016 [==============================] - 8s 282us/step - loss: 0.2797\n",
      "Epoch 46/100\n",
      "29016/29016 [==============================] - 8s 282us/step - loss: 0.2743\n",
      "Epoch 47/100\n",
      "29016/29016 [==============================] - 8s 281us/step - loss: 0.2692\n",
      "Epoch 48/100\n",
      "29016/29016 [==============================] - 8s 280us/step - loss: 0.2642\n",
      "Epoch 49/100\n",
      "29016/29016 [==============================] - 8s 280us/step - loss: 0.2597\n",
      "Epoch 50/100\n",
      "29016/29016 [==============================] - 8s 280us/step - loss: 0.2552\n",
      "Epoch 51/100\n",
      "29016/29016 [==============================] - 8s 278us/step - loss: 0.2507\n",
      "Epoch 52/100\n",
      "29016/29016 [==============================] - 8s 277us/step - loss: 0.2466\n",
      "Epoch 53/100\n",
      "29016/29016 [==============================] - 8s 277us/step - loss: 0.2426\n",
      "Epoch 54/100\n",
      "29016/29016 [==============================] - 8s 278us/step - loss: 0.2385\n",
      "Epoch 55/100\n",
      "29016/29016 [==============================] - 8s 277us/step - loss: 0.2348\n",
      "Epoch 56/100\n",
      "29016/29016 [==============================] - 8s 277us/step - loss: 0.2312\n",
      "Epoch 57/100\n",
      "29016/29016 [==============================] - 8s 277us/step - loss: 0.2278\n",
      "Epoch 58/100\n",
      "29016/29016 [==============================] - 8s 278us/step - loss: 0.2246\n",
      "Epoch 59/100\n",
      "29016/29016 [==============================] - 8s 277us/step - loss: 0.2211\n",
      "Epoch 60/100\n",
      "29016/29016 [==============================] - 8s 278us/step - loss: 0.2179\n",
      "Epoch 61/100\n",
      "29016/29016 [==============================] - 8s 277us/step - loss: 0.2148\n",
      "Epoch 62/100\n",
      "29016/29016 [==============================] - 8s 278us/step - loss: 0.2117\n",
      "Epoch 63/100\n",
      "29016/29016 [==============================] - 8s 276us/step - loss: 0.2086\n",
      "Epoch 64/100\n",
      "29016/29016 [==============================] - 8s 277us/step - loss: 0.2058\n",
      "Epoch 65/100\n",
      "29016/29016 [==============================] - 8s 277us/step - loss: 0.2028\n",
      "Epoch 66/100\n",
      "29016/29016 [==============================] - 8s 278us/step - loss: 0.2001\n",
      "Epoch 67/100\n",
      "29016/29016 [==============================] - 8s 277us/step - loss: 0.1974\n",
      "Epoch 68/100\n",
      "29016/29016 [==============================] - 8s 277us/step - loss: 0.1949\n",
      "Epoch 69/100\n",
      "29016/29016 [==============================] - 8s 277us/step - loss: 0.1925\n",
      "Epoch 70/100\n",
      "29016/29016 [==============================] - 8s 277us/step - loss: 0.1903\n",
      "Epoch 71/100\n",
      "29016/29016 [==============================] - 8s 277us/step - loss: 0.1880\n",
      "Epoch 72/100\n",
      "29016/29016 [==============================] - 8s 277us/step - loss: 0.1860\n",
      "Epoch 73/100\n",
      "29016/29016 [==============================] - 8s 277us/step - loss: 0.1839\n",
      "Epoch 74/100\n",
      "29016/29016 [==============================] - 8s 278us/step - loss: 0.1818\n",
      "Epoch 75/100\n",
      "29016/29016 [==============================] - 8s 278us/step - loss: 0.1798\n",
      "Epoch 76/100\n",
      "29016/29016 [==============================] - 8s 277us/step - loss: 0.1778\n",
      "Epoch 77/100\n",
      "29016/29016 [==============================] - 8s 277us/step - loss: 0.1759\n",
      "Epoch 78/100\n",
      "29016/29016 [==============================] - 8s 276us/step - loss: 0.1741\n",
      "Epoch 79/100\n",
      "29016/29016 [==============================] - 8s 276us/step - loss: 0.1723\n",
      "Epoch 80/100\n",
      "29016/29016 [==============================] - 8s 278us/step - loss: 0.1706\n",
      "Epoch 81/100\n",
      "29016/29016 [==============================] - 8s 278us/step - loss: 0.1688\n",
      "Epoch 82/100\n",
      "29016/29016 [==============================] - 8s 277us/step - loss: 0.1675\n",
      "Epoch 83/100\n",
      "29016/29016 [==============================] - 8s 277us/step - loss: 0.1658\n",
      "Epoch 84/100\n",
      "29016/29016 [==============================] - 8s 277us/step - loss: 0.1643\n",
      "Epoch 85/100\n",
      "29016/29016 [==============================] - 8s 276us/step - loss: 0.1628\n",
      "Epoch 86/100\n",
      "29016/29016 [==============================] - 8s 277us/step - loss: 0.1614\n",
      "Epoch 87/100\n",
      "29016/29016 [==============================] - 8s 277us/step - loss: 0.1599\n",
      "Epoch 88/100\n",
      "29016/29016 [==============================] - 8s 277us/step - loss: 0.1589\n",
      "Epoch 89/100\n",
      "29016/29016 [==============================] - 8s 277us/step - loss: 0.1575\n",
      "Epoch 90/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29016/29016 [==============================] - 8s 276us/step - loss: 0.1564\n",
      "Epoch 91/100\n",
      "29016/29016 [==============================] - 8s 277us/step - loss: 0.1548\n",
      "Epoch 92/100\n",
      "29016/29016 [==============================] - 8s 277us/step - loss: 0.1538\n",
      "Epoch 93/100\n",
      "29016/29016 [==============================] - 8s 277us/step - loss: 0.1526\n",
      "Epoch 94/100\n",
      "29016/29016 [==============================] - 8s 277us/step - loss: 0.1513\n",
      "Epoch 95/100\n",
      "29016/29016 [==============================] - 8s 277us/step - loss: 0.1503\n",
      "Epoch 96/100\n",
      "29016/29016 [==============================] - 8s 277us/step - loss: 0.1492\n",
      "Epoch 97/100\n",
      "29016/29016 [==============================] - 8s 277us/step - loss: 0.1483\n",
      "Epoch 98/100\n",
      "29016/29016 [==============================] - 8s 277us/step - loss: 0.1471\n",
      "Epoch 99/100\n",
      "29016/29016 [==============================] - 8s 276us/step - loss: 0.1462\n",
      "Epoch 100/100\n",
      "29016/29016 [==============================] - 8s 277us/step - loss: 0.1449\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcc392dfcf8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese_net.fit(train_pairs, train_targets, batch_size = 64, epochs = 100, verbose = 1)\n",
    "#cnn_model.fit(X_train.reshape(118880, 1001, 1), y_train.values, batch_size = 64, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创立训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n",
      "(2, 32, 1001, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "#trainfile=\"/home/suliangbu/work/wanghong/train_low_None_.pickle\"\n",
    "file_to_read = open('train_low_None_.pickle', 'rb')\n",
    "tmp = pickle.load(file_to_read)\n",
    "data = tmp.values\n",
    "\n",
    "featur = np.zeros((186, 800, 1001))\n",
    "for i in range(186):\n",
    "    featur[i] = data[i * 800:(i+1) * 800, :1001]\n",
    "\n",
    "def get_batch(batch_size,featur):\n",
    "    \"\"\"Create batch of n pairs, half same class, half different class\"\"\"\n",
    "    n_examples = 800\n",
    "    n_classes = 186\n",
    "\n",
    "    \n",
    "    categories = np.random.choice(n_classes,size=(batch_size,),replace=False)\n",
    "    \n",
    "    pairs = [np.zeros((batch_size, 1001, 1)) for i in range(2)]\n",
    "    \n",
    "    targets=np.zeros((batch_size))\n",
    "\n",
    "    targets[batch_size//2:] = 1\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "\n",
    "        category = categories[i]\n",
    "        idx_1 = np.random.randint(0, n_examples)        \n",
    "        pairs[0][i] = featur[category, idx_1].reshape(1001,1)\n",
    "\n",
    "        idx_2 = np.random.randint(0, n_examples)\n",
    "\n",
    "        if i >= batch_size // 2:\n",
    "            category_2 = category  \n",
    "        else: \n",
    "            category_2 = (category + np.random.randint(1,n_classes)) % n_classes\n",
    "\n",
    "        pairs[1][i] = featur[category_2,idx_2].reshape(1001,1)\n",
    "    #pairs = np.array(pairs)\n",
    "    \n",
    "    #print(pairs.shape)\n",
    "    return pairs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ZIF-67' 'ZIF-67' 'ZIF-67' 'ZIF-67' 'ZIF-71' 'ZIF-71' 'ZIF-71' 'ZIF-8'\n",
      " 'ZIF-8' 'ZIF-8' 'ZIF-8' 'ZIF-90' 'ZIF-90' 'ZIF-90']\n",
      "(14, 2252)\n"
     ]
    }
   ],
   "source": [
    "file_to_read = open('T_origin.pickle', 'rb')\n",
    "\n",
    "tmp = pickle.load(file_to_read)\n",
    "\n",
    "test_data = tmp.values\n",
    "\n",
    "print(test_data[:,-1])\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.load('mix_testdata_and_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_targets, test_pairs = [], []\n",
    "pair_1,pair_2 = [], []\n",
    "for i in range(14):\n",
    "    for j in range(i + 1, 14):\n",
    "        if test_data[i][-1] == test_data[j][-1]:\n",
    "            pair_1.append(test_data[i][:-1])\n",
    "            pair_2.append(test_data[j][:-1])\n",
    "            #test_pairs.append([test_data[i][:-1], test_data[j][:-1]])\n",
    "            test_targets.append(1)\n",
    "        else:\n",
    "            #test_pairs.append([test_data[i][:-1], test_data[j][:-1]])\n",
    "            pair_1.append(test_data[i][:-1])\n",
    "            pair_2.append(test_data[j][:-1])\n",
    "            test_targets.append(0)\n",
    "test_pairs = [pair_1,pair_2]\n",
    "test_pairs = np.array(test_pairs).reshape(2,91,2251,1)\n",
    "test_pairs = test_pairs.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = open('test_pairs.pickle', 'wb')\n",
    "\n",
    "pickle.dump(test_pairs,output)\n",
    "\n",
    "output.close()\n",
    "\n",
    "output1 = open('test_targets.pickle', 'wb')\n",
    "\n",
    "pickle.dump(test_targets,output1)\n",
    "\n",
    "output1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创立测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.033514307811291076, 0.02062374323279156, 0.027068832173240076,\n",
       "        ..., 0.037638437741685626, 0.054137664346480555, 'ZIF-67'],\n",
       "       [0.03475991590433889, 0.04274393096490252, 0.02536397301273244,\n",
       "        ..., 0.569748107565793, 0.6209495144208277, 'ZIF-67'],\n",
       "       [0.032538760318275455, 0.04601976130386533, 0.042300527867580276,\n",
       "        ..., 0.036722374982263904, 0.043695066088909475, 'ZIF-67'],\n",
       "       ...,\n",
       "       [0.01178765033802156, 0.013248283739033317, 0.0131714002046804,\n",
       "        ..., 0.0027931685785534794, 0.0014350260564532555, 'ZIF-90'],\n",
       "       [0.09375007508678823, 0.11250007065970596, 0.10717590525013387,\n",
       "        ..., 0.014351836889150319, 0.02152786991703112, 'ZIF-71'],\n",
       "       [0.11164413672968, 0.11295253299787038, 0.10989987003826307, ...,\n",
       "        0.02224155251981437, 0.02616669553280636, 'ZIF-71']], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_to_read = open('test__None_.pickle', 'rb')\n",
    "\n",
    "tmp = pickle.load(file_to_read)\n",
    "print(tmp)\n",
    "test_data = tmp.values  \n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 409682 into shape (2,91,1002,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e8d451c9e3d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# test_pairs = np.array(test_pairs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# print(test_pairs.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtest_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_pairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m91\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1002\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mtest_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_pairs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#test_targets = np.array(test_targets)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 409682 into shape (2,91,1002,1)"
     ]
    }
   ],
   "source": [
    "test_targets, test_pairs = [], []\n",
    "pair_1,pair_2 = [], []\n",
    "for i in range(14):\n",
    "    for j in range(i + 1, 14):\n",
    "        if test_data[i][-1] == test_data[j][-1]:\n",
    "            pair_1.append(test_data[i][:-1])\n",
    "            pair_2.append(test_data[j][:-1])\n",
    "            #test_pairs.append([test_data[i][:-1], test_data[j][:-1]])\n",
    "            test_targets.append(1)\n",
    "        else:\n",
    "            #test_pairs.append([test_data[i][:-1], test_data[j][:-1]])\n",
    "            pair_1.append(test_data[i][:-1])\n",
    "            pair_2.append(test_data[j][:-1])\n",
    "            test_targets.append(0)\n",
    "test_pairs = [pair_1,pair_2]\n",
    "# test_pairs = np.array(test_pairs)\n",
    "# print(test_pairs.shape)\n",
    "test_pairs = np.array(test_pairs).reshape(2,91,1002,1)\n",
    "test_pairs = test_pairs.tolist()\n",
    "#test_targets = np.array(test_targets)\n",
    "#test_pairs.shape\n",
    "#print(pairs,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ZIF-67']\n",
      "['ZIF-71']\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(test_pairs[0][3][-1][:])\n",
    "print(test_pairs[1][3][-1][:])\n",
    "print(test_targets[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 500, training loss: 0.37,\n",
      "iteration 1000, training loss: 0.38,\n",
      "iteration 1500, training loss: 0.40,\n",
      "iteration 2000, training loss: 0.35,\n",
      "iteration 2500, training loss: 0.40,\n",
      "iteration 3000, training loss: 0.30,\n",
      "iteration 3500, training loss: 0.30,\n",
      "iteration 4000, training loss: 0.31,\n",
      "iteration 4500, training loss: 0.33,\n",
      "iteration 5000, training loss: 0.32,\n",
      "iteration 5500, training loss: 0.28,\n",
      "iteration 6000, training loss: 0.26,\n",
      "iteration 6500, training loss: 0.30,\n",
      "iteration 7000, training loss: 0.28,\n",
      "iteration 7500, training loss: 0.23,\n",
      "iteration 8000, training loss: 0.30,\n",
      "iteration 8500, training loss: 0.31,\n",
      "iteration 9000, training loss: 0.23,\n",
      "iteration 9500, training loss: 0.35,\n",
      "iteration 10000, training loss: 0.27,\n",
      "iteration 10500, training loss: 0.24,\n",
      "iteration 11000, training loss: 0.23,\n",
      "iteration 11500, training loss: 0.23,\n",
      "iteration 12000, training loss: 0.27,\n",
      "iteration 12500, training loss: 0.22,\n",
      "iteration 13000, training loss: 0.20,\n",
      "iteration 13500, training loss: 0.24,\n",
      "iteration 14000, training loss: 0.24,\n",
      "iteration 14500, training loss: 0.27,\n",
      "iteration 15000, training loss: 0.18,\n",
      "iteration 15500, training loss: 0.23,\n",
      "iteration 16000, training loss: 0.21,\n",
      "iteration 16500, training loss: 0.17,\n",
      "iteration 17000, training loss: 0.19,\n",
      "iteration 17500, training loss: 0.18,\n",
      "iteration 18000, training loss: 0.15,\n",
      "iteration 18500, training loss: 0.17,\n",
      "iteration 19000, training loss: 0.19,\n",
      "iteration 19500, training loss: 0.16,\n",
      "iteration 20000, training loss: 0.16,\n",
      "iteration 20500, training loss: 0.15,\n",
      "iteration 21000, training loss: 0.15,\n",
      "iteration 21500, training loss: 0.16,\n",
      "iteration 22000, training loss: 0.14,\n",
      "iteration 22500, training loss: 0.17,\n",
      "iteration 23000, training loss: 0.22,\n",
      "iteration 23500, training loss: 0.20,\n",
      "iteration 24000, training loss: 0.14,\n",
      "iteration 24500, training loss: 0.17,\n",
      "iteration 25000, training loss: 0.13,\n",
      "iteration 25500, training loss: 0.18,\n",
      "iteration 26000, training loss: 0.13,\n",
      "iteration 26500, training loss: 0.14,\n",
      "iteration 27000, training loss: 0.12,\n",
      "iteration 27500, training loss: 0.13,\n",
      "iteration 28000, training loss: 0.13,\n",
      "iteration 28500, training loss: 0.16,\n",
      "iteration 29000, training loss: 0.20,\n",
      "iteration 29500, training loss: 0.13,\n",
      "iteration 30000, training loss: 0.12,\n",
      "iteration 30500, training loss: 0.12,\n",
      "iteration 31000, training loss: 0.11,\n",
      "iteration 31500, training loss: 0.11,\n",
      "iteration 32000, training loss: 0.11,\n",
      "iteration 32500, training loss: 0.12,\n",
      "iteration 33000, training loss: 0.13,\n",
      "iteration 33500, training loss: 0.11,\n",
      "iteration 34000, training loss: 0.13,\n",
      "iteration 34500, training loss: 0.11,\n",
      "iteration 35000, training loss: 0.15,\n",
      "iteration 35500, training loss: 0.11,\n",
      "iteration 36000, training loss: 0.10,\n",
      "iteration 36500, training loss: 0.14,\n",
      "iteration 37000, training loss: 0.11,\n",
      "iteration 37500, training loss: 0.16,\n",
      "iteration 38000, training loss: 0.10,\n",
      "iteration 38500, training loss: 0.10,\n",
      "iteration 39000, training loss: 0.10,\n",
      "iteration 39500, training loss: 0.11,\n",
      "iteration 40000, training loss: 0.10,\n",
      "iteration 40500, training loss: 0.10,\n",
      "iteration 41000, training loss: 0.10,\n",
      "iteration 41500, training loss: 0.09,\n",
      "iteration 42000, training loss: 0.10,\n",
      "iteration 42500, training loss: 0.10,\n",
      "iteration 43000, training loss: 0.09,\n",
      "iteration 43500, training loss: 0.10,\n",
      "iteration 44000, training loss: 0.11,\n",
      "iteration 44500, training loss: 0.09,\n",
      "iteration 45000, training loss: 0.09,\n",
      "iteration 45500, training loss: 0.15,\n",
      "iteration 46000, training loss: 0.19,\n",
      "iteration 46500, training loss: 0.09,\n",
      "iteration 47000, training loss: 0.09,\n",
      "iteration 47500, training loss: 0.09,\n",
      "iteration 48000, training loss: 0.09,\n",
      "iteration 48500, training loss: 0.19,\n",
      "iteration 49000, training loss: 0.09,\n",
      "iteration 49500, training loss: 0.09,\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "loss_every = 500\n",
    "for i in range(1,50000):\n",
    "    (inputs,targets)=get_batch(batch_size,featur)\n",
    "    loss=siamese_net.train_on_batch(inputs,targets)\n",
    "    #print(loss)\n",
    "    if i % loss_every == 0:\n",
    "        print(\"iteration {}, training loss: {:.2f},\".format(i,loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.908702  ]\n",
      " [0.67001295]\n",
      " [0.955983  ]\n",
      " [0.7392195 ]\n",
      " [0.689074  ]\n",
      " [0.6867306 ]\n",
      " [0.46435875]\n",
      " [0.49303654]\n",
      " [0.3723563 ]\n",
      " [0.43139482]\n",
      " [0.33008108]\n",
      " [0.35192215]\n",
      " [0.90542334]\n",
      " [0.43762007]\n",
      " [0.8949132 ]\n",
      " [0.52020395]\n",
      " [0.45589662]\n",
      " [0.4579588 ]\n",
      " [0.3002059 ]\n",
      " [0.31243443]\n",
      " [0.17407334]\n",
      " [0.3856165 ]\n",
      " [0.28850293]\n",
      " [0.30817986]\n",
      " [0.9666454 ]\n",
      " [0.7373388 ]\n",
      " [0.8235408 ]\n",
      " [0.87644434]\n",
      " [0.87577933]\n",
      " [0.87243366]\n",
      " [0.87154067]\n",
      " [0.8699995 ]\n",
      " [0.7475954 ]\n",
      " [0.70706505]\n",
      " [0.71304613]\n",
      " [0.4359071 ]\n",
      " [0.79201925]\n",
      " [0.74945396]\n",
      " [0.74882287]\n",
      " [0.54229724]\n",
      " [0.5586129 ]\n",
      " [0.43171555]\n",
      " [0.48004955]\n",
      " [0.38380855]\n",
      " [0.403131  ]\n",
      " [0.8919996 ]\n",
      " [0.9500402 ]\n",
      " [0.954239  ]\n",
      " [0.60935235]\n",
      " [0.60994065]\n",
      " [0.776379  ]\n",
      " [0.4014426 ]\n",
      " [0.3447327 ]\n",
      " [0.33781812]\n",
      " [0.5121361 ]\n",
      " [0.9669719 ]\n",
      " [0.71483064]\n",
      " [0.7164275 ]\n",
      " [0.85543597]\n",
      " [0.51757944]\n",
      " [0.45765305]\n",
      " [0.4511677 ]\n",
      " [0.45006043]\n",
      " [0.70925546]\n",
      " [0.71246177]\n",
      " [0.84416074]\n",
      " [0.51413196]\n",
      " [0.45352882]\n",
      " [0.445031  ]\n",
      " [0.45454037]\n",
      " [0.9520977 ]\n",
      " [0.85309064]\n",
      " [0.9167371 ]\n",
      " [0.91402733]\n",
      " [0.9147552 ]\n",
      " [0.29942468]\n",
      " [0.8568405 ]\n",
      " [0.92279136]\n",
      " [0.9078439 ]\n",
      " [0.9064268 ]\n",
      " [0.3119985 ]\n",
      " [0.6903637 ]\n",
      " [0.6692175 ]\n",
      " [0.66296226]\n",
      " [0.17330393]\n",
      " [0.9444257 ]\n",
      " [0.9530835 ]\n",
      " [0.3886807 ]\n",
      " [0.96018606]\n",
      " [0.29269943]\n",
      " [0.31228328]]\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "51.64835164835165\n"
     ]
    }
   ],
   "source": [
    "probs = siamese_net.predict(test_pairs)\n",
    "n_correct = 0\n",
    "print(probs)\n",
    "for i in range(len(probs)):\n",
    "    if probs[i] <= 0.5:\n",
    "        probs[i] = 0 \n",
    "    else:\n",
    "        probs[i] = 1\n",
    "print(probs)\n",
    "\n",
    "for i in range(len(probs)):\n",
    "    if probs[i] == test_targets[i]:\n",
    "        n_correct += 1 \n",
    "percent_correct = (100.0*n_correct / 91)\n",
    "print(percent_correct)\n",
    "    \n",
    "    \n",
    "    \n",
    "#    if np.argmax(probs) == np.argmax(test_targets):\n",
    "#        n_correct+=1\n",
    "#        percent_correct = (100.0*n_correct / 91)\n",
    "#print(percent_correct)\n",
    "#print(len(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.64835164835165\n"
     ]
    }
   ],
   "source": [
    "print(percent_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "12\n",
      "14\n",
      "15\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "81\n",
      "82\n",
      "83\n",
      "85\n",
      "86\n",
      "88\n"
     ]
    }
   ],
   "source": [
    "# print(probs)\n",
    "for i in range(91):\n",
    "    if(probs[i]==1):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-5bdbc21a21f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(14):\n",
    "    print(test_data[i][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "13\n",
      "14\n",
      "25\n",
      "46\n",
      "47\n",
      "55\n",
      "70\n",
      "71\n",
      "72\n",
      "76\n",
      "77\n",
      "81\n",
      "88\n",
      "89\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "for i in range(91):\n",
    "    if(test_targets[i]==1):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import h5py\n",
    "\n",
    "# model = siamese_net\n",
    "# model.save('model_12_18.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yanghaoyue/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/yanghaoyue/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/yanghaoyue/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# file_to_read = open('new_test_pairs.pickle', 'rb')\n",
    "\n",
    "# test_pairs = pickle.load(file_to_read)\n",
    "\n",
    "\n",
    "# file_to_read = open('new_test_targets.pickle', 'rb')\n",
    "\n",
    "# test_targets = pickle.load(file_to_read)\n",
    "\n",
    "\n",
    "from keras.models import load_model\n",
    " \n",
    "siamese_net= load_model('model_12_18.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[array([0.0, 0.0, 0.0, ..., 8.27511, 6.47198, 5.21193], dtype=object), array([0.0, 0.0, 0.0, ..., 8.27511, 6.47198, 5.21193], dtype=object), array([0.0, 0.0, 0.0, ..., 8.27511, 6.47198, 5.21193], dtype=object), array([0.0, 0.0, 0.0, ..., 8.27511, 6.47198, 5.21193], dtype=object)], [array([2276.67, 2320.0, 2140.0, ..., 2300.0, 2393.33, 2410.0],\n",
      "      dtype=object), array([2070.0, 2286.67, 2176.67, ..., 2080.0, 2106.67, 2150.0],\n",
      "      dtype=object), array([1456.67, 1293.33, 1456.67, ..., 2393.33, 2386.67, 2496.67],\n",
      "      dtype=object), array([1010.0, 1095.0, 1220.0, ..., 1050.0, 1135.0, 1080.0], dtype=object)]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'tolist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a0fb2d12cc28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_pairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_pairs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_pairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'tolist'"
     ]
    }
   ],
   "source": [
    "print(test_pairs)\n",
    "test_pairs = test_pairs.tolist()\n",
    "print(test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
